{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckbbj5diaHkg"
      },
      "source": [
        "# Fine-tuning Embeddings for RAG on Specific Data\n",
        "\n",
        "As we start our \"fine-tuning\" week, we'll start with the lowest hanging improvement one can do for RAG - which is:\n",
        "\n",
        "Fine-tuning embeddings!\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  - Task 1: Dependencies and Boilerplate\n",
        "  - Task 2: Loading Data\n",
        "  - Task 3: Constructing a Fine-tuning Dataset\n",
        "  - Task 4: Fine-tuning `snowflake-arctic-embed-l`\n",
        "  - Task 5: Evaluating our Retriever\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xwor_3X6ODX"
      },
      "source": [
        "#### Basic Overview of Fine-tuning Embeddings\n",
        "\n",
        "In essence, what we want to do when we fine-tune our embedding models is very simple:\n",
        "\n",
        "```\n",
        "Move the embeddings for questions relating to a document\n",
        "closer together with that document\n",
        "```\n",
        "\n",
        "We can think of fine-tuning our embedding models as follows:\n",
        "\n",
        "1) We have some pair of text items that *should* be closer together\n",
        "  - `Question`, `Document` pairs\n",
        "  - EX: `Who drives the bus?`, `The bus was driven by Kyle, the Bus Driver`.\n",
        "\n",
        "2) We use these pairs as labeled data to fine-tune our embedding model.\n",
        "\n",
        "The process of training helps the model more accurately associate our questions with the correct documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX5R3HVz6FOQ"
      },
      "source": [
        "##### ‚ùì Question #1:\n",
        "\n",
        "Describe the nuance between using Q&D pairs to train the embedding model vs. inter-document pairs/related sentences.\n",
        "\n",
        "What caveats does this approach have? Are there any special considerations for what kind of Q's we should use?\n",
        "\n",
        "<span style=\"color:green\"> Q&D (Question & Document/Answer): This is the approach we're taking in this notebook. We generate questions that should be answered by a specific chunk of text (our \"document\" or context). The goal is to train the model to make the embedding for the question very similar (close in vector space) to the embedding of the document chunk that answers it. This directly optimizes for the core RAG task: finding the right context for a given question.\n",
        "\n",
        "<span style=\"color:green\">Inter-document Pairs/Related Sentences: This approach involves identifying pairs of sentences or document chunks that are inherently related or similar without necessarily being a question-answer pair. For example, two paragraphs discussing the same specific concept, or a statement and its elaboration. Training on these pairs teaches the model general semantic similarity ‚Äì making embeddings for related content closer together.\n",
        "\n",
        "<span style=\"color:green\">The Q&D pair approach is more targeted because it directly mimics the core task of a RAG system: retrieving relevant context (D) based on a user's query (Q). By training the model to pull question embeddings closer to their corresponding answer/context embeddings, **we are explicitly optimizing the retriever for its intended function within the RAG pipeline.**\n",
        "\n",
        "<span style=\"color:green\">The effectiveness of this fine-tuning heavily relies on the representativeness of the generated questions. If the synthetic questions we create for training don't reflect the types of questions users will actually ask the RAG system, the fine-tuning might not translate into real-world performance improvements.\n",
        "\n",
        "\n",
        "<span style=\"color:green\">If The LLM generates very simple pointed questions for our fine tuning and the users end up asking more nuanced thematic questions, our fine tuning will not help with that. LLM is also big brained and one-size-fits-all so it is more general and our use case might be specific so **its important that our fine tuning q and a set captures the specificity of our use case without being tied to the exact wording (that will make the embeddings too brittle) and that it captures the nuance and complexity of the domain as well.** So, well formed, relevant, specific to domain without being brittle, and representative of what users will ask.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NkSaurzbpyS"
      },
      "source": [
        "## Task 1: Dependencies and Boilerplate\n",
        "\n",
        "We'll set up our `nest_asyncio` so we can leverage async loops in our Notebook.\n",
        "\n",
        "We'll also install the required libraries we'll be using today, and set up our OpenAI API key!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_EUibmcDU3"
      },
      "source": [
        "### Nest Asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zq-6s7LbPnKH"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8uFz8RVcFFu"
      },
      "source": [
        "### Install Dependencies\n",
        "\n",
        ">> NOTE: You do not need to do these steps if you are running this notebook locally with `uv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulZIBA1ZoSsV"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain_openai langchain_huggingface langchain_core langchain langchain_community langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GFD7B-tOCrx"
      },
      "outputs": [],
      "source": [
        "!pip install -qU faiss-cpu python-pptx==1.0.2 nltk==3.9.1 pymupdf beautifulsoup4 lxml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FM-eUlrcI8a"
      },
      "source": [
        "### Provide OpenAI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA_mlurVqtrp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFZ217gCDVTr"
      },
      "source": [
        "## Task 2: Loading Data\n",
        "\n",
        "We'll prepare our data - and download our webpages which we'll be using for our data today.\n",
        "\n",
        "These webpages are from [Simon Willison's](https://simonwillison.net/) yearly \"AI learnings\".\n",
        "\n",
        "- [2023 Blog](https://simonwillison.net/2023/Dec/31/ai-in-2023/)\n",
        "- [2024 Blog](https://simonwillison.net/2024/Dec/31/llms-in-2024/)\n",
        "\n",
        "Let's start by collecting our data into a useful pile!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSoqUqKeXkWR"
      },
      "outputs": [],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh9b7aJ7XkWR"
      },
      "outputs": [],
      "source": [
        "!curl https://simonwillison.net/2023/Dec/31/ai-in-2023/ -o data/2023_llms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP6vNjx0XkWS"
      },
      "outputs": [],
      "source": [
        "!curl https://simonwillison.net/2024/Dec/31/llms-in-2024/ -o data/2024_llms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHJhTzsvN75t"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "\n",
        "path = \"data/\"\n",
        "text_loader = DirectoryLoader(path, glob=\"*.html\", loader_cls=BSHTMLLoader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UbKa6-V0nvp"
      },
      "source": [
        "Next, we'll set up a classic naive chunking strategy as we only care that the documents get parsed into chunks that we can generate synthetic questions about.\n",
        "\n",
        "<span style=\"color:green\">Chunk size:\n",
        "\n",
        "<span style=\"color:green\">Too Small: Chunks might lack sufficient context to fully capture an idea or answer a question. The embedding might not be specific enough.\n",
        "\n",
        "<span style=\"color:green\">Too Large: The embedding might become too general, averaging the meaning across multiple distinct points. When retrieved, this large chunk might contain the relevant info but also a lot of noise, potentially making it harder for the final LLM to pinpoint the answer (diluting the key information).\n",
        "\n",
        "<span style=\"color:green\">The 750-character size with 20-character overlap is a common starting point, aiming for that balance. The RecursiveCharacterTextSplitter is helpful because it tries to split along natural boundaries (paragraphs, sentences) first before resorting to a hard character limit, which helps keep the chunks more coherent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsPrOOqXOsNX"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 750,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf_PoX7l09Rg"
      },
      "source": [
        "Next we can load/split these documents as follows.\n",
        "\n",
        "> NOTE: You may need to run this cell twice to get it to work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMYPX6N6Os8M"
      },
      "outputs": [],
      "source": [
        "training_documents = text_splitter.split_documents(text_loader.load())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAozuMoNOvnp"
      },
      "outputs": [],
      "source": [
        "len(training_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yE2TFIq1BuJ"
      },
      "source": [
        "Next, we're going to associate each of our chunks with a unique identifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwyIForybIpo"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "id_set = set()\n",
        "\n",
        "for document in training_documents:\n",
        "  id = str(uuid.uuid4())\n",
        "  while id in id_set:\n",
        "    id = uuid.uuid4()\n",
        "  id_set.add(id)\n",
        "  document.metadata[\"id\"] = id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJnL4oNg341U"
      },
      "source": [
        "Next, we'll simply use naive Python slicing to create a training, test, and validation set to prepare our data for the next step. \\\n",
        "<span style=\"color:green\">78 chunks: training\\\n",
        "<span style=\"color:green\">12 chunks: validation\\\n",
        "<span style=\"color:green\">12 chunks: testing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvczX7VuXkWS"
      },
      "source": [
        "<span style=\"color:green\">**Validation vs. Test Sets:**\n",
        "\n",
        "<span style=\"color:green\">The training set is used directly to update the model's parameters during the fine-tuning process. The model \"learns\" from these examples.\n",
        "\n",
        "<span style=\"color:green\">The validation set plays a crucial role **during** training. Periodically (e.g., after each epoch or a certain number of steps), the model's performance is checked against the validation set. This helps us:\n",
        "\n",
        "<span style=\"color:green\">Tune hyperparameters: See if different learning rates, batch sizes, etc., lead to better performance on data the model hasn't directly trained on.\n",
        "\n",
        "<span style=\"color:green\">Prevent overfitting: Monitor if the model is getting really good at the training data but worse on the validation data (meaning it's just memorizing the training set and not generalizing). We can use this to decide when to stop training (early stopping).\n",
        "\n",
        "<span style=\"color:green\">Model selection: If we train multiple versions of the model, the validation set helps us pick the best one.\n",
        "\n",
        "<span style=\"color:green\">The test set is held back until the very end. After we've finished training and selected our final model (using the training and validation sets), we evaluate its performance one last time on the test set. This gives us an unbiased estimate of how well the model is likely to perform on completely new, unseen data in the real world. We don't use the test set to make any decisions about training or model selection; it's purely for the final report card."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTS4GTSEcnG4"
      },
      "outputs": [],
      "source": [
        "training_split_documents = training_documents[:len(training_documents) - 24]\n",
        "val_split_documents = training_documents[len(training_documents) - 24:len(training_documents) - 12]\n",
        "test_split_documents = training_documents[len(training_documents) - 12:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzlvKbONDWvQ"
      },
      "source": [
        "## Task 3: Constructing a Fine-tuning Dataset\n",
        "\n",
        "Using the nodes we created above, we can finally start constructing a fine-tuning dataset utilizing OpenAI's `gpt-4.1-mini`\n",
        "\n",
        "The basic idea here is straightforward enough:\n",
        "\n",
        "1. We look at a document\n",
        "2. We generate questions that could be answered by that node\n",
        "\n",
        "This gives us a number of question/context pairs that we can use to fine-tune our Embeddings model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbPzMxFZXkWT"
      },
      "source": [
        "<span style=\"color:green\">Why temperature=0?\n",
        "\n",
        "<span style=\"color:green\">In LLMs, temperature controls the randomness of the output.\n",
        "\n",
        "<span style=\"color:green\">A higher temperature (e.g., 0.7-1.0) makes the output more random and creative. The model is more likely to explore less probable word choices.\n",
        "\n",
        "<span style=\"color:green\">A lower temperature (closer to 0) makes the output more deterministic and focused. The model tends to pick the most likely next word.\n",
        "\n",
        "<span style=\"color:green\">When generating questions for our fine-tuning dataset, we want them to be factual, directly based on the provided context, and consistent.\n",
        "\n",
        "<span style=\"color:green\">We don't want creative or unexpected questions here. Setting temperature=0 helps ensure the LLM produces the most probable, focused, and contextually grounded questions, minimizing randomness and increasing reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EWfmIscMrvg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "qa_chat_model = ChatOpenAI(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-hLnsSB6Y-S"
      },
      "source": [
        "We'll create a simple Question Generation prompt to query `gpt-4o-mini` to generate Questions for each retrieved context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diEWcw00NMSj"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "qa_prompt = \"\"\"\\\n",
        "Given the following context, you must generate questions based on only the provided context.\n",
        "\n",
        "You are to generate {n_questions} questions which should be provided in the following format:\n",
        "\n",
        "1. QUESTION #1\n",
        "2. QUESTION #2\n",
        "...\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "qa_prompt_template = ChatPromptTemplate.from_template(qa_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u87Izpgm6_fk"
      },
      "source": [
        "We'll create a simple chain to query the LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggl9SSjiNbpG"
      },
      "outputs": [],
      "source": [
        "question_generation_chain = qa_prompt_template | qa_chat_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4duvHirh7DQv"
      },
      "source": [
        "There's a lot going on in this function - let's take a deeper look:\n",
        "\n",
        "1. First, we provide a list of documents and a number of questions\n",
        "2. We, for each document in our list, generate `n_questions` of questions.\n",
        "3. We then associate those questions and contexts via a `UUID`.\n",
        "\n",
        "> NOTE: The reason we're doing this `UUID` association is for ease of use later in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lm2JvgC9X37"
      },
      "source": [
        "##### üèóÔ∏è Activity #1:\n",
        "\n",
        "<span style=\"color:green\">**DONE** Vibe Coded :)\n",
        "\n",
        "We have:\n",
        "\n",
        "- Lists of `Documents` with the `metadata` field `id`.\n",
        "\n",
        "We need:\n",
        "\n",
        "- An object with key `id`, which have values `str` questions.\n",
        "- An object with key `question_id`, which have values `List(str)` which will be a list of associated `context_id`.\n",
        "\n",
        "An Example:\n",
        "\n",
        "question_object:\n",
        "```python\n",
        "{\n",
        "'b4b95fb6-f827-4454-aa5b-20e62733f172': 'What types of accessible formats are available for persons with disabilities?',\n",
        "'df58ee4f-714c-419e-8324-94e5870574e2': 'How do accessible formats benefit persons with disabilities?',\n",
        "'505fce8b-0e56-48de-a251-61027e396918': 'What are some of the risks associated with the increasing capabilities of AI systems that generate synthetic content?',\n",
        "'8ff0ab33-60dc-4fee-8958-91bfb686aca8': 'Why is it important for providers of AI systems to embed technical solutions for marking and detecting synthetic content?'\n",
        "}\n",
        " ```\n",
        "\n",
        " context_object:\n",
        " ```python\n",
        "{\n",
        "'b4b95fb6-f827-4454-aa5b-20e62733f172': ['dd75bf94-75f3-4603-8e4b-5522f6925638'],\n",
        "'df58ee4f-714c-419e-8324-94e5870574e2': ['dd75bf94-75f3-4603-8e4b-5522f6925638'],\n",
        "'505fce8b-0e56-48de-a251-61027e396918': ['ffe3893f-688c-48e8-90bd-7a9feb953d90'],\n",
        "'8ff0ab33-60dc-4fee-8958-91bfb686aca8': ['ffe3893f-688c-48e8-90bd-7a9feb953d90'],\n",
        "}\n",
        " ```\n",
        "\n",
        " As you can see, a piece of context can be associated with more than 1 question.\n",
        "\n",
        " The task is to write the Python function(s) to accomplish this task.\n",
        "\n",
        " Your function signature is provided below, along with the desired return values.\n",
        "\n",
        " > NOTE: You can make any modifications that you desire - assuming that you have the correct input and outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4yi4NfTCnLc"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import asyncio\n",
        "import re    # Import regex for parsing questions\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Sample Usage of TQDM:\n",
        "\n",
        "for i in tqdm.tqdm(range(10)):\n",
        "  time.sleep(1)\n",
        "\"\"\"\n",
        "\n",
        "async def create_questions_old(documents, n_questions):\n",
        "\n",
        "    questions = {}\n",
        "    relevant_docs = {}\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    return questions, relevant_docs\n",
        "\n",
        "\n",
        "async def create_questions(documents, n_questions):\n",
        "    questions = {}\n",
        "    relevant_docs = {}\n",
        "\n",
        "    tasks = []\n",
        "    # Prepare async tasks for each document\n",
        "    for document in documents:\n",
        "        context = document.page_content\n",
        "        doc_id = document.metadata[\"id\"]\n",
        "        # Create a coroutine for each document processing\n",
        "        task = process_document(context, doc_id, n_questions, question_generation_chain)\n",
        "        tasks.append(task)\n",
        "\n",
        "    # Run tasks concurrently with progress bar\n",
        "    results = []\n",
        "    for future in tqdm.tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Generating Questions\"):\n",
        "        try:\n",
        "            result = await future\n",
        "            if result:\n",
        "                results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing document: {e}\") # Basic error logging\n",
        "\n",
        "    # Process results to populate the dictionaries\n",
        "    for q_list, doc_id in results:\n",
        "        for question_text in q_list:\n",
        "            question_id = str(uuid.uuid4())\n",
        "            # Add the check for collisions\n",
        "            while question_id in questions:\n",
        "                question_id = str(uuid.uuid4()) # Regenerate if collision\n",
        "            questions[question_id] = question_text\n",
        "            relevant_docs[question_id] = [doc_id] # Store doc_id in a list as per example\n",
        "\n",
        "    return questions, relevant_docs\n",
        "\n",
        "async def process_document(context, doc_id, n_questions, chain):\n",
        "    \"\"\"Helper coroutine to process a single document.\"\"\"\n",
        "    # Invoke the LLM chain to generate questions\n",
        "    response = await chain.ainvoke({\"context\": context, \"n_questions\": n_questions})\n",
        "\n",
        "    # Basic parsing assuming \"1. QUESTION\\n2. QUESTION\\n...\" format\n",
        "    # Use regex to find lines starting with number and dot\n",
        "    # Adjust regex if the format is slightly different\n",
        "    parsed_questions = re.findall(r\"^\\d+\\.\\s*(.*)\", response.content, re.MULTILINE)\n",
        "\n",
        "    # Fallback or alternative parsing if needed\n",
        "    if not parsed_questions:\n",
        "         # Try splitting by newline if regex fails (less robust)\n",
        "         parsed_questions = [q.strip() for q in response.content.strip().split('\\n') if q.strip()]\n",
        "         # Filter out potential non-question lines if necessary (heuristic)\n",
        "         parsed_questions = [q for q in parsed_questions if len(q) > 10 and '?' in q] # Example filter\n",
        "\n",
        "    # Ensure we don't exceed n_questions, even if LLM gave more/less\n",
        "    # Or handle cases where fewer than n_questions were generated\n",
        "    final_questions = parsed_questions[:n_questions]\n",
        "\n",
        "    if not final_questions:\n",
        "        print(f\"Warning: No questions parsed for doc_id {doc_id}. Raw response: {response.content[:100]}...\")\n",
        "        return None # Return None if no questions could be parsed\n",
        "\n",
        "    return final_questions, doc_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W0eWOUo4QGL"
      },
      "source": [
        "### REMOVE `await` IF NOT USING ASYNC (HINT: Use `async`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85Dq6KRqEs0F"
      },
      "outputs": [],
      "source": [
        "training_questions, training_relevant_contexts = await create_questions(training_split_documents, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FSTG0bb7w73"
      },
      "source": [
        "We'll use the function to generate training, validation, and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIZm4CqGVzBx"
      },
      "outputs": [],
      "source": [
        "val_questions, val_relevant_contexts = await create_questions(val_split_documents, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6qUHg9sV2_y"
      },
      "outputs": [],
      "source": [
        "test_questions, test_relevant_contexts = await create_questions(test_split_documents, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqRuTct_XkWf"
      },
      "outputs": [],
      "source": [
        "# Lets see some questions!\n",
        "test_questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_jYOnAI43zK"
      },
      "source": [
        "### <span style=\"color:green\">Reformatting and Saving Datasets\n",
        "\n",
        "<span style=\"color:green\">Now, we can save our datasets for later use!\n",
        "We save these generated datasets, along with the original document chunks (the \"corpus\"), into JSON files: training_dataset.jsonl, val_dataset.jsonl, and test_dataset.jsonl.\n",
        "\n",
        "\n",
        "\n",
        " <span style=\"color:green\">We're saving the train_dataset, val_dataset, and test_dataset dictionaries as JSON files (using json.dump) for several key reasons:\n",
        "\n",
        "<span style=\"color:green\">Human Readability: JSON (JavaScript Object Notation) is a text-based format that is relatively easy for humans to read and understand, especially compared to binary formats. This makes it simple to inspect the generated questions and contexts if needed.\n",
        "\n",
        "<span style=\"color:green\">Interoperability: JSON is a language-independent data format. While we're using Python now, saving in JSON means the data could be easily loaded and used by programs written in other languages if necessary in the future.\n",
        "Ease of Use in Python: Python's built-in json library makes it trivial to serialize (write) Python dictionaries and lists to a JSON file (json.dump) and deserialize (read) them back into Python objects (json.load) later in the notebook or in a different script.\n",
        "\n",
        "<span style=\"color:green\">Structured Data: JSON naturally represents the kind of nested structures we have (dictionaries containing other dictionaries and lists), mapping well to our Python objects.\n",
        "Essentially, it's a standard, convenient, and readable way to store structured data like our question-answer pairs and corpus for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF6IFFq9VsNu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# training-split-documents is the original document chunks. It is a LangChain Document object. Convert it to a\n",
        "# dictionary with the document id as the key and the page content as the value.\n",
        "training_corpus = {train_item.metadata[\"id\"] : train_item.page_content for train_item in training_split_documents}\n",
        "\n",
        "# training_questions is a dictionary with the question id as the key and the question as the value.\n",
        "# training_relevant_contexts is a dictionary with the question id as the key and the document id as the value.\n",
        "# training_corpus is a dictionary with the document id as the key and the page content as the value.\n",
        "train_dataset = {\n",
        "    \"questions\" : training_questions,\n",
        "    \"relevant_contexts\" : training_relevant_contexts,\n",
        "    \"corpus\" : training_corpus\n",
        "}\n",
        "\n",
        "# Save the training dataset to a JSON file\n",
        "with open(\"training_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(train_dataset, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqF9WaueV-V8"
      },
      "outputs": [],
      "source": [
        "val_corpus = {val_item.metadata[\"id\"] : val_item.page_content for val_item in val_split_documents}\n",
        "\n",
        "val_dataset = {\n",
        "    \"questions\" : val_questions,\n",
        "    \"relevant_contexts\" : val_relevant_contexts,\n",
        "    \"corpus\" : val_corpus\n",
        "}\n",
        "\n",
        "with open(\"val_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(val_dataset, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DSQ7WMnWAu6"
      },
      "outputs": [],
      "source": [
        "train_corpus = {test_item.metadata[\"id\"] : test_item.page_content for test_item in test_split_documents}\n",
        "\n",
        "test_dataset = {\n",
        "    \"questions\" : test_questions,\n",
        "    \"relevant_contexts\" : test_relevant_contexts,\n",
        "    \"corpus\" : train_corpus\n",
        "}\n",
        "\n",
        "with open(\"test_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(test_dataset, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAwklqQCgVi-"
      },
      "source": [
        "## Task 4: Fine-tuning `snowflake-arctic-embed-l`\n",
        "\n",
        "Now that we have a dataset, let's grab a `sentence-transformers` Embeddings model!\n",
        "\n",
        "We'll be using Snowflake's [`snowflake-arctic-embed-l`](https://huggingface.co/Snowflake/snowflake-arctic-embed-l) as a base embeddings model.\n",
        "\n",
        "<span style=\"color:green\">So, we're essentially taking a knowledgeable generalist and turning it into a specialist for our particular task and data. This way, we use less data and compute, we leverage existing knowledge of a pre trained model, and we do focused learning.<span style=\"color:green\">\n",
        "\n",
        "<span style=\"color:green\">It is a well performing embeddings model by itself, but there's a lot of very specific domain terms and vocabulary in our courpus - so lets fine-tune it and see what that can do for us!\n",
        "\n",
        ">> NOTE: Skip installing dependencies if you are running this notebook locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXzVHP3v1Cno"
      },
      "outputs": [],
      "source": [
        "!pip install -qU sentence_transformers datasets pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-PGsQB7Xo6V"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_id = \"Snowflake/snowflake-arctic-embed-l\"\n",
        "model = SentenceTransformer(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ztG07iB8CFO"
      },
      "source": [
        "We'll grab some necessary imports from `sentence_transformers` and `torch`.\n",
        "\n",
        "> NOTE: PyTorch (`torch`) is a popular machine learning library - while we don't go very deep into PyTorch it's an incredibly powerful and interesting library! Please read more about it [here](https://pytorch.org/tutorials/beginner/basics/intro.html)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-WbpuUWYFJr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from sentence_transformers import InputExample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJtPPlck8HBE"
      },
      "source": [
        "We're using a toy batch size here to reflect the limited number of examples we have. We have 78 training documents, and two questions each so all of 156 ques/doc pairs or examples.\n",
        "\n",
        "> NOTE: It is typical to use a much larger batch size (~64+), hardware permitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Lokhy6KYHAv"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-6DT8hc8PmT"
      },
      "source": [
        "Let's move our dataset into the expected format for training.\n",
        "\n",
        "<span style=\"color:green\">Remember we have query_ids, doc_ids, and query text and doc text. Now, we want query_text, doc_text examples. We go from query_id to doc_id to doc_text\n",
        "\n",
        "<span style=\"color:green\">The loss function (which we'll see next) will use these pairs to calculate how \"far apart\" the query and text embeddings currently are and generate gradients to push them closer together.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJk37zQsYJ4P"
      },
      "outputs": [],
      "source": [
        "corpus = train_dataset['corpus']\n",
        "queries = train_dataset['questions']\n",
        "relevant_docs = train_dataset['relevant_contexts']\n",
        "\n",
        "examples = []\n",
        "for query_id, query in queries.items():\n",
        "    doc_id = relevant_docs[query_id][0]\n",
        "    text = corpus[doc_id]\n",
        "    example = InputExample(texts=[query, text])\n",
        "    examples.append(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjFx7KHI8TL0"
      },
      "source": [
        "Now we can create a `torch` `DataLoader`!\n",
        "\n",
        "<span style=\"color:green\">**Shuffling**: The DataLoader, by default, shuffles the training examples at the beginning of each epoch (each full pass through the data). This is crucial to prevent the model from learning any patterns based on the order in which examples happen to appear in the dataset. If the data wasn't shuffled, the model might inadvertently learn biases related to the sequence, which could hurt its ability to generalize to new, unseen data. Shuffling ensures it sees examples in a random order each time.\n",
        "\n",
        "<span style=\"color:green\"><span style=\"color:green\">**Batching:** (our size=10)\n",
        "\n",
        "<span style=\"color:green\">Efficiency: 10 at a time in parallel. And Gradient Stability: We compute error and direction every batch size and update gradient. Smoother and more relaible training.\n",
        "\n",
        "<span style=\"color:green\"><span style=\"color:green\">**Within One Epoch**:\n",
        "\n",
        "  - <span style=\"color:green\">Get the next 10 examples.\n",
        "  - <span style=\"color:green\">Forward pass: Compute embeddings for these\n",
        "  - <span style=\"color:green\">Calculate loss. A score to see how well the model did. Specifically, it measures if the related query-context pairs are closer together in embedding space than unrelated pairs within that same batch.\n",
        "  - <span style=\"color:green\">Calculate gradients: Based on loss, how much each parameter contributed to the loss. (backward pass) - these are gradients.\n",
        "  - <span style=\"color:green\">Update parametes (gradient adjustment): An \"optimizer\" uses these gradients to slightly adjust the model's parameters (weights) to try and reduce the loss next time. This \"adjustment\" happens after processing each batch.\n",
        "  - <span style=\"color:green\">Repeat: Steps 1-5 are repeated for the next batch, and the next, until all the training examples have been seen once.\n",
        "\n",
        "  \n",
        "<span style=\"color:green\">**Gradient Stability:** Imagine updating the model based on just one query-context pair. That single example might be weird or unrepresentative, causing the parameter update (gradient adjustment) to be jerky or point in a slightly wrong direction. By calculating the loss and gradients over a batch (10 examples), the \"weirdness\" of individual examples tends to average out. The resulting gradient provides a more stable, reliable estimate of the direction the parameters should move to improve performance on average across those 10 examples. This usually leads to smoother, more consistent training.\n",
        "\n",
        "\n",
        "<span style=\"color:green\">**Epochs and Validation:**\n",
        "An epoch is defined as one complete pass through the entire training dataset. Since we have 156 training examples and a batch size of 10, one epoch consists of ceil(156 / 10) = 16 batches (15 batches of 10, and one final batch of 6).\n",
        "\n",
        "<span style=\"color:green\">**Validation** typically happens after each epoch (as specified by evaluation_strategy=\"epoch\" which is often the default, or evaluation_steps=50 as explicitly set later in cell 60). The model is put into evaluation mode (no gradients calculated, no parameters updated), and its performance is measured on the separate validation set. This gives us an idea of how well the model is generalizing to data it hasn't been trained on during that epoch.\n",
        "\n",
        "<span style=\"color:green\">So, to summarize: Learning (loss calculation, gradient adjustment) happens per batch. An epoch is a full pass over all batches. Validation is a separate check, usually done between epochs, to monitor generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiizmeIqZ_-w"
      },
      "outputs": [],
      "source": [
        "loader = DataLoader(\n",
        "    examples, batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vA8rzlX8XbT"
      },
      "source": [
        "**Moving on!**\n",
        "Next up, we'll prepare our loss function!\n",
        "\n",
        "Loss is an important part of training, fine-tuning, and more. If you want a deep dive on loss - you can check out our [event on loss!](https://www.youtube.com/watch?v=iB8FWR9aD5Q&t=8s).\n",
        "\n",
        "The core loss we're using today is called `MultipleNegativesRankingLoss` - you can find more information [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py).\n",
        "\n",
        "This is \"wrapped\" in `MatryoshkaLoss`, which you can read the implementation of [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MatryoshkaLoss.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uga4nnBqlVeh"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
        "\n",
        "matryoshka_dimensions = [768, 512, 256, 128, 64]\n",
        "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
        "train_loss = MatryoshkaLoss(\n",
        "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJG4fOm66PHI"
      },
      "source": [
        "##### üèóÔ∏è Activity #2:\n",
        "\n",
        "Both of these losses sound \"cool\", but what are they - exactly - under the hood?\n",
        "\n",
        "Why are these losses specifically doing? Please write a short summary of each loss.\n",
        "\n",
        "\n",
        "Okay, here's a summary of the two loss functions, incorporating our discussion:\n",
        "\n",
        "#### <span style=\"color:green\">Loss Functions Explained\n",
        "\n",
        "#### <span style=\"color:green\">1. MultipleNegativesRankingLoss (MNRL)\n",
        "\n",
        "*   <span style=\"color:green\">**Core Goal:** The primary objective of MNRL is to fine-tune the embedding model so that the vector embedding of a query is semantically closer to the embedding of its relevant context (the \"positive\" pair) than it is to the embeddings of irrelevant contexts (the \"negative\" pairs). In simpler terms, it pushes related items together and unrelated items apart in the embedding space.\n",
        "*   <span style=\"color:green\">**Mechanism - In-Batch Negatives:** This loss function cleverly avoids the need to explicitly provide negative examples. When processing a batch of (query, positive_context) pairs, it uses an \"in-batch negative\" strategy. For a specific query (Query_A) in the batch:\n",
        "    *   Its corresponding PositiveContext_A is treated as the single positive example.\n",
        "    *   All *other* contexts present in that same batch (PositiveContext_B, PositiveContext_C, etc.) are implicitly treated as *negative* examples for Query_A.\n",
        "*   <span style=\"color:green\">**Training Signal:** The loss is calculated based on how well the model ranks the similarity score of the positive pair (sim(Query_A, PositiveContext_A)) compared to the similarity scores of the negative pairs (sim(Query_A, PositiveContext_B), sim(Query_A, PositiveContext_C), etc.). The goal is to maximize the positive similarity relative to the negative similarities within that batch.\n",
        "*   <span style=\"color:green\">**Batch Size Dependency:** The effectiveness of this strategy relies on the batch size. A larger batch provides more (and potentially harder) negative examples for each query, generally leading to more robust training.\n",
        "\n",
        "#### <span style=\"color:green\">2. MatryoshkaLoss\n",
        "\n",
        "*   <span style=\"color:green\">**Core Goal:** This loss function aims to train embeddings that are not only effective at their full dimensionality but also perform well when truncated to shorter lengths (like Russian nesting dolls). The primary motivation is **efficiency** in downstream applications like RAG ‚Äì shorter embeddings require less storage, faster retrieval computations, and less bandwidth.\n",
        "*   <span style=\"color:green\">**Mechanism - Learning Hierarchical Structure:** MatryoshkaLoss achieves this by incentivizing the model *during training* to learn a hierarchical representation within the embedding vector. It encourages the model to pack the most crucial, coarse-grained semantic information into the initial dimensions of the vector, adding progressively finer-grained details in subsequent dimensions.\n",
        "*   <span style=\"color:green\">**Training Process:**\n",
        "    1.  It \"wraps\" an inner loss function (in our case, MultipleNegativesRankingLoss).\n",
        "    2.  For each batch, it calculates the inner loss multiple times: once using the full-dimension embeddings (e.g., 768), and then again using only the first N dimensions for each specified shorter length (e.g., first 512, first 256, first 128, first 64).\n",
        "    3.  These individual loss values (calculated at different dimensionalities) are combined, often using a weighted average. This combined loss reflects how well the embedding performs *at multiple levels of truncation*.\n",
        "    4.  The model's parameters are updated based on the gradient of this *combined* loss.\n",
        "    5.  By penalizing poor performance at shorter lengths *during the training loop*, this mechanism forces the model to organize information hierarchically, ensuring the truncated versions remain meaningful.\n",
        "*   <span style=\"color:green\">**Outcome:** The result is an embedding model where the initial dimensions capture the most vital information. This *enables* practitioners, after training, to evaluate performance at different truncation levels (e.g., 768 vs. 512 vs. 256) and choose the best trade-off between accuracy and efficiency (storage/speed) for their specific RAG application. The MatryoshkaLoss during training is what makes this post-training choice possible and meaningful.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVi0k92rXkWg"
      },
      "source": [
        "<span style=\"color:green\">**A note on creating negative context examples.**\n",
        " We created two queries for each context. So, it might incorrectly pick a context as negative for a query when it isn't. Gemini says this is not such a big deal:\n",
        "\n",
        "<span style=\"color:green\">Primary Goal: The loss function's main goal is to ensure sim(q1, c) is higher than sim(q1, other_context) for all other contexts in the batch. Even if one of those \"other contexts\" happens to be the same text c (but paired with q2), the loss still pushes to maximize the similarity for the direct (q1, c) pairing relative to everything else.\n",
        "\n",
        "<span style=\"color:green\">Different Queries: While the context c is the same text, q1 and q2 are (hopefully) different questions. The model learns to associate the specific semantics of q1 with c and the specific semantics of q2 with c. Treating c (paired with q2) as a negative for q1 encourages the model to differentiate why c is relevant specifically to q1 compared to other potential queries (like q2).\n",
        "\n",
        "<span style=\"color:green\">Batch Size/Probability: With shuffling and reasonable batch sizes, the chances of both pairs derived from the exact same context landing in the same batch decrease, though it can certainly happen.\n",
        "\n",
        "<span style=\"color:green\">In practice, this nuance of the in-batch negative strategy usually doesn't prevent the model from learning effectively, especially since the positive pairing signal is strong and consistent across batches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKxRuXfH844c"
      },
      "source": [
        "Now we can set-up our evaluator.\n",
        "\n",
        "> NOTE: Due to the formatting of our dataset - this is all we have to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0hAFwUyaHQG"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "\n",
        "corpus = val_dataset['corpus']\n",
        "queries = val_dataset['questions']\n",
        "relevant_docs = val_dataset['relevant_contexts']\n",
        "\n",
        "evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYfap_ct8-bU"
      },
      "source": [
        "We'll train this model for 5 epochs, though you could increase this number if we had a significant amount more data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svZG0pBHiQr6"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxitWoNX9DwW"
      },
      "source": [
        "It's training time!\n",
        "\n",
        "> NOTE: We're manually defining a warm-up period here - this is just to provide a smooth ramp into our training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6Q7o02cXkWh"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.init(mode=\"disabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_bZN8YEXkWh"
      },
      "source": [
        "> NOTE: You may not see direct improvement during the training cycles - this is absolutely expected. We will verify performance later in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDhUHZY-iR09"
      },
      "outputs": [],
      "source": [
        "warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
        "\n",
        "model.fit(\n",
        "    train_objectives=[(loader, train_loss)],\n",
        "    epochs=EPOCHS,\n",
        "    warmup_steps=warmup_steps,\n",
        "    output_path='finetuned_arctic_ft',\n",
        "    show_progress_bar=True,\n",
        "    evaluator=evaluator,\n",
        "    evaluation_steps=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3iwclvyRD8L"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pn-Y6yjRoHk"
      },
      "outputs": [],
      "source": [
        "hf_username = <<\"YOUR_HF_USERNAME\">>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqhf3zWa9AiJ"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "model.push_to_hub(f\"{hf_username}/legal-ft-{uuid.uuid4()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVa2Ac2zPvC9"
      },
      "source": [
        "https://huggingface.co/geetach/legal-ft-a201f63a-cb7a-4d10-aa78-6229827dff89/commit/257e07b0661736f960d468f10defdd94211ef448"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bo0zW5k9Poq"
      },
      "source": [
        "## Task 5: Evaluating our Retriever\n",
        "\n",
        "Now that we have fine-tuned our retriever - let's see if it's worthwhile!\n",
        "\n",
        "We'll start with some basic imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vq-2oqU0wHFr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_core.documents import Document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jD0qrIh9X8f"
      },
      "source": [
        "Now we'll define a function that will help us evaluate our retrieval process.\n",
        "\n",
        "> NOTE: We're assuming 1 correct document in a \"hit\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0713_3cowX4q"
      },
      "outputs": [],
      "source": [
        "def evaluate_openai(\n",
        "    dataset,\n",
        "    embed_model,\n",
        "    top_k=5,\n",
        "    verbose=False,\n",
        "):\n",
        "  corpus = dataset['corpus']\n",
        "  questions = dataset['questions']\n",
        "  relevant_docs = dataset['relevant_contexts']\n",
        "  documents = [Document(page_content=content, metadata={\"id\": doc_id}) for doc_id, content in corpus.items()]\n",
        "  vectorstore = FAISS.from_documents(documents, embed_model)\n",
        "\n",
        "  retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
        "\n",
        "  eval_results = []\n",
        "  for id, question in tqdm.tqdm(questions.items()):\n",
        "    retrieved_nodes = retriever.invoke(question)\n",
        "    retrieved_ids = [node.metadata[\"id\"] for node in retrieved_nodes]\n",
        "    expected_id = relevant_docs[id][0]\n",
        "    is_hit = expected_id in retrieved_ids\n",
        "    eval_results.append({\"id\": id, \"question\": question, \"expected_id\": expected_id, \"is_hit\": is_hit})\n",
        "\n",
        "  return eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOr49m4O9lxY"
      },
      "source": [
        "All that's left to do is evaluate, we'll evaluate our model against:\n",
        "\n",
        "1. OpenAI's closed source `text-embedding-3-small`\n",
        "2. The base non-fine-tuned version of `Snowflake/snowflake-arctic-embed-l`.\n",
        "\n",
        "Let's see how it stacks up!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijaeYpf593IW"
      },
      "source": [
        "### `text-embedding-3-small`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyY3PztaxnU3"
      },
      "outputs": [],
      "source": [
        "te3_openai = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "te3_results = evaluate_openai(test_dataset, te3_openai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkyW90TCxx_i"
      },
      "outputs": [],
      "source": [
        "te3_results_df = pd.DataFrame(te3_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MscVRdNCylJ-"
      },
      "outputs": [],
      "source": [
        "te3_hit_rate = te3_results_df[\"is_hit\"].mean()\n",
        "te3_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ra-mh0L96dQ"
      },
      "source": [
        "### `Snowflake/snowflake-arctic-embed-l` (base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEskxwvFypHe"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-l\")\n",
        "arctic_embed_m_results = evaluate_openai(test_dataset, huggingface_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlKgiXTWzMTg"
      },
      "outputs": [],
      "source": [
        "arctic_embed_m_results_df = pd.DataFrame(arctic_embed_m_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zV5vJWrJzOhc"
      },
      "outputs": [],
      "source": [
        "arctic_embed_m_hit_rate = arctic_embed_m_results_df[\"is_hit\"].mean()\n",
        "arctic_embed_m_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcR3-0s19_lu"
      },
      "source": [
        "### `Snowflake/snowflake-arctic-embed-l` (fine-tuned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ilse1LduzP1i"
      },
      "outputs": [],
      "source": [
        "finetune_embeddings = HuggingFaceEmbeddings(model_name=\"finetuned_arctic_ft\")\n",
        "finetune_results = evaluate_openai(test_dataset, finetune_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxhZPqkNzZlh"
      },
      "outputs": [],
      "source": [
        "finetune_results_df = pd.DataFrame(finetune_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4thAK2BXzaj6"
      },
      "outputs": [],
      "source": [
        "finetune_hit_rate = finetune_results_df[\"is_hit\"].mean()\n",
        "finetune_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iegFM209mBk3"
      },
      "source": [
        "## Task 1: Vibe Checking the RAG Pipeline\n",
        "\n",
        "We're going to use our RAG pipeline to vibe check on some common phrases now that we've modified it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzg0AA5krgR4"
      },
      "source": [
        "### Creating New Chunks\n",
        "\n",
        "In order to try and evaluate our system more fairly, let's create new chunks that we will use to create our Vector Store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwQ2_LqNr0Tw"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 600,\n",
        "    chunk_overlap  = 50,\n",
        "    length_function = len\n",
        ")\n",
        "\n",
        "training_documents = text_splitter.split_documents(text_loader.load())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIdxahHXpP-c"
      },
      "source": [
        "### Base Chain\n",
        "\n",
        "We'll start by constructing our base chain, which will use the untrained retrieval model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOsxIXpNpWC2"
      },
      "source": [
        "#### R - Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azIGIKYfmNCT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-l\")\n",
        "base_vectorstore = FAISS.from_documents(training_documents, huggingface_embeddings)\n",
        "base_retriever = base_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-1nVZ0KpX5N"
      },
      "source": [
        "#### A - Augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G10Fr-aKojeA"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "Given a provided context and a question, you must answer the question. If you do not know the answer, you must state that you do not know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt_template = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Euq6RQEopZvD"
      },
      "source": [
        "#### G - Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-mfbbrypMHG"
      },
      "outputs": [],
      "source": [
        "rag_llm =  ChatOpenAI(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ2p4mnUpbYY"
      },
      "source": [
        "#### RAG - LCEL RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssuR-LaboyGq"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "\n",
        "base_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emm6WbB9pfKt"
      },
      "outputs": [],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is an agent?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUOrd0OBprAq"
      },
      "outputs": [],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"Who has produced better models than GPT-3?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnfuFl59py7I"
      },
      "outputs": [],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is the laziest time of the year for AI?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NmqwHBDqTZ8"
      },
      "outputs": [],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is the largest model that Simon has run on his phone?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqNS0UJAp3lC"
      },
      "source": [
        "### Fine-tuned Embedding Model\n",
        "\n",
        "Now let's rebuild our RAG chain with the Fine-tuned model - the only component we need to change is our `FAISS` vectorstore!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el_4axkbKxsR"
      },
      "source": [
        "**To avoid re-running the fine tuning when I reload this into Google Colab, I am getting the fine tuned model I pushed to HuggingFace**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihO7tP6mqATy"
      },
      "outputs": [],
      "source": [
        "# If you just ran the fine tuning, you can use the following line to load the fine tuned model.\n",
        "finetune_embeddings = HuggingFaceEmbeddings(model_name=\"finetuned_arctic_ft\")\n",
        "\n",
        "# Otherwise, you can use the following line to load the fine tuned model from HuggingFace. From your old runs.\n",
        "# finetune_embeddings = HuggingFaceEmbeddings(model_name=\"geetach/legal-ft-450c1026-6554-476b-96f1-34f426f777c8\")\n",
        "\n",
        "finetune_vectorstore = FAISS.from_documents(training_documents, finetune_embeddings)\n",
        "finetune_retriever = finetune_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_cIFvWzqKGY"
      },
      "outputs": [],
      "source": [
        "finetune_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | finetune_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJmRHJF2qNgj"
      },
      "outputs": [],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"What is an Agent?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnK-c2ugqPPh"
      },
      "outputs": [],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"Who has produced better models than GPT-3?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83hssg1AWozc"
      },
      "outputs": [],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"What is the laziest time of the year for AI?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsHmGeFbqRET"
      },
      "outputs": [],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"What is the largest model that Simon has run on his phone?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDgD8seY_I3W"
      },
      "source": [
        "#### ‚ùìQuestion #2:\n",
        "\n",
        "Which LCEL RAG Chain do you think answered the questions better, and why?\n",
        "\n",
        "<span style=\"color:green\"> The fine tuned rag chain!! Here is why:\n",
        "\n",
        "<span style=\"color:green\"> **Specificity and Contextual Relevance**\n",
        "\n",
        "  - <span style=\"color:green\">For questions like \"What is an agent?\" or \"What is the laziest time of the year for AI?\", the fine-tuned chain is more likely to retrieve the specific passages from Simon Willison's blogs discussing these topics in his specific context (e.g., LLM agents, the ChatGPT \"lazy\" period).\n",
        "  - <span style=\"color:green\">The base model, relying on more general embeddings, might retrieve less relevant chunks or provide more generic answers that miss the nuances present in the source documents.\n",
        "  - <span style=\"color:green\">Similarly, for specific factual questions like \"What is the largest model that Simon has run on his phone?\", the fine-tuned embeddings are much more likely to correctly identify and retrieve the exact text containing this information, whereas the base model might struggle.\n",
        "\n",
        "<span style=\"color:green\"> **More on evaluation**\n",
        "  - <span style=\"color:green\"> If you look at the hit rate, we can see that the fine tuned retireiver outperformed the base retriever. - its better at finding the correct chunk for a given synhtetic question.\n",
        "  -- <span style=\"color:green\"> Later, I did a langsmith evaluation of these two rag chains and the fine tuned rag chain achieved better scores for both correctness and helpfulness (screenshot at the end of this notebook!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCbq1sZArIx4"
      },
      "source": [
        "## <span style=\"color:green\">Task 2: RAGAS Evaluation\n",
        "\n",
        "It's great to have some idea of how our system is doing based on vibe-checks, but let's use RAGAS to provide more insight info. on how things are improving!\n",
        "\n",
        "> NOTE: Please recreate *exactly* the RAGAS process we used to evaluate RAG, baselining with the default retriever, and then comparing the new retriever. The includes the Synthetic Data Generation steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd_CmvttXkWj"
      },
      "source": [
        "<span style=\"color:green\">Ok, so we already have the base_rag_chain and finetune_rag_chain. We want to now generate test data via ragas and then evaluate the two rag chains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq880DtHk9pX"
      },
      "outputs": [],
      "source": [
        "!pip install -qU ragas==0.2.10\n",
        "!pip install unstructured"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhJsO4qVXkWj"
      },
      "source": [
        "Load our data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzAwAd5sXkWj"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "path = \"data/\"\n",
        "loader = DirectoryLoader(path, glob=\"*.html\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JodPMkxQXkWj"
      },
      "source": [
        "<span style=\"color:green\">**Generate synthetic test data using ragas.** We provide an llm, an embedding model, and testset_size, and our documents and RAGAS does it's magic via knowledge graphs!\n",
        "\n",
        "<span style=\"color:green\"><span style=\"color:green\">As a reminder, this will create about 10 rows of test data with the following columns:\\\n",
        "**user_input\treference_contexts\treference\tsynthesizer_name**\\\n",
        "User_input is the query, reference_context is the ideal context for this query, reference is the ideal response for this query and synthesizer name is the query synthesizer used. It will be one of SingleHopSpecific, MultiHopSpecific, and MultiHopAbstract."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37iz3MB6XkWj"
      },
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "sdg_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "sdg_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
        "sdg_generator = TestsetGenerator(llm=sdg_llm, embedding_model=sdg_embeddings)\n",
        "sdg_dataset = sdg_generator.generate_with_langchain_docs(docs, testset_size=10)\n",
        "\n",
        "# Make a copy for the fine tuned model.\n",
        "finetuned_sdg_dataset = sdg_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS38pZy9XkWk"
      },
      "source": [
        "<span style=\"color:green\">**Evaluate the UnFineTuned and FineTuned model with Ragas**\n",
        "\n",
        "<span style=\"color:green\">**First, we create the two datasets, by adding the retrieved response and retrieved context** - we do this for both the rag chains, one which uses the base snowflake embedding model and one which uses the finetuned snowflake embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lEUkh3QXkWk"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "for test_row in sdg_dataset:\n",
        "  response = base_rag_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"]\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "  time.sleep(5)\n",
        "\n",
        "time.sleep(5)\n",
        "\n",
        "sdg_dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVAutZ-8XkWk"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "for test_row in finetuned_sdg_dataset:\n",
        "  response = finetune_rag_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"]\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "  time.sleep(5)\n",
        "\n",
        "time.sleep(5)\n",
        "\n",
        "finetuned_sdg_dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv9JqR-GXkWk"
      },
      "source": [
        "<span style=\"color:green\">**Convert our two datasets into a RAGAS evaluation dataset using a ragas library function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qq3E4MSPXkWk"
      },
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset\n",
        "\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(sdg_dataset.to_pandas())\n",
        "finetuned_evaluation_dataset = EvaluationDataset.from_pandas(finetuned_sdg_dataset.to_pandas())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgJwNvkdXkWk"
      },
      "source": [
        "<span style=\"color:green\">**Set our evaluation LLM and evaluation config**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZzzMg4eXkWk"
      },
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
        "from ragas import evaluate, RunConfig\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))\n",
        "\n",
        "# default max_workers is 16.\n",
        "# custom_run_config = RunConfig(timeout=360, max_workers=8)\n",
        "custom_run_config = RunConfig(\n",
        "    timeout=300,          # 5 minutes max for operations\n",
        "    max_retries=15,       # More retries for rate limits\n",
        "    max_wait=90,          # Longer wait between retries\n",
        "    max_workers=8,        # Fewer concurrent API calls\n",
        "    log_tenacity=True     # Log retry attempts\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AzPEprEXkWk"
      },
      "source": [
        "<span style=\"color:green\">**Evaluate the Base model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fZrD-__XkWk"
      },
      "outputs": [],
      "source": [
        "base_result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "base_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwV6gO8AXkWk"
      },
      "source": [
        "<span style=\"color:green\">**Evaluate the Finetuned model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh4qoQ16XkWk"
      },
      "outputs": [],
      "source": [
        "finetuned_result = evaluate(\n",
        "    dataset=finetuned_evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "finetuned_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwbnfhthKxsT"
      },
      "source": [
        "<span style=\"color:green\">**RAGAS Evaluation Results: PERPLEXING? NOT!! We used a less powerful LLM Model. Since the last time I did this with gpt-4.1, it cost me!!**\n",
        "\n",
        "<span style=\"color:green\"> The RAGAS evaluation presented mixed results across the two runs as you can see in my tables below. While some metrics like context_recall or noise_sensitivity improved with fine-tuning in one or both runs, others like faithfulness or context_entity_recall sometimes decreased.  So, what do we learn from this:\n",
        "  - <span style=\"color:green\">Evaluation metrics can be sensitive, and RAGAS itself relies on LLMs for judgments, introducing variability.\n",
        "  - <span style=\"color:green\">However, the consistent qualitative improvement and the positive LangSmith results suggest an overall benefit from fine-tuning in this case.\n",
        "\n",
        "<span style=\"color:green\">**Run Two**\n",
        "\n",
        "| Metric | Base Model | Finetuned Model|\n",
        "| :------------------------- | :----- | :-------- |\n",
        "| context_recall | 0.5913 | 0.6329 |\n",
        "| faithfulness | 0.8048 | 0.7716 |\n",
        "| factual_correctness | 0.4783 | 0.4508 |\n",
        "| answer_relevancy | 0.6075 | 0.6073 |\n",
        "| context_entity_recall | 0.5012 | 0.5123 |\n",
        "| noise_sensitivity_relevant | 0.3838 | 0.3917 |\n",
        "\n",
        "<span style=\"color:green\">**Run One**\n",
        "\n",
        "| Metric | Base Model | Finetuned model |\n",
        "| :------------------------- | :--------- | :-------------- |\n",
        "| context_recall | 0.5819 | 0.5864 |\n",
        "| faithfulness | 0.8220 | 0.7950 |\n",
        "| factual_correctness | 0.3150 | 0.3427 |\n",
        "| answer_relevancy | 0.6332 | 0.6332 |\n",
        "| context_entity_recall | 0.4674 | 0.4431 |\n",
        "| noise_sensitivity_relevant | 0.5305 | 0.6317 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5787_QrKxsT"
      },
      "source": [
        "<span style=\"color:green\">**OK, Lets try this using LangSmith correctness and helpfulness evaluators we learned in Assignment 7**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DM8bfIeNiBEn"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ[\"LANGCHAIN_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Assignment 10 LangSmith Trial2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xzPS6eng3aH"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"EmbeddingEval2 10 State of AI Across the Years!\"\n",
        "\n",
        "langsmith_dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Assignment 10 try 2 State of AI Across the Years!\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVCjmPxtjIWp"
      },
      "outputs": [],
      "source": [
        "# Convert the dataset from Ragas format to LangSmith format.\n",
        "for data_row in sdg_dataset.to_pandas().iterrows():\n",
        "  client.create_example(\n",
        "      inputs={\n",
        "          \"question\": data_row[1][\"user_input\"]\n",
        "      },\n",
        "      outputs={\n",
        "          \"answer\": data_row[1][\"reference\"]\n",
        "      },\n",
        "      metadata={\n",
        "          \"context\": data_row[1][\"reference_contexts\"]\n",
        "      },\n",
        "      dataset_id=langsmith_dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ3PsHNbjZSW"
      },
      "outputs": [],
      "source": [
        "eval_llm = ChatOpenAI(model=\"gpt-4.1\")\n",
        "\n",
        "def prep_data(run, example):\n",
        "    return {\n",
        "        \"prediction\": run.outputs['response'],  # Map 'response' key to 'prediction'\n",
        "        \"reference\": example.outputs['answer'], # Map 'answer' key to 'reference'\n",
        "        \"input\": example.inputs['question'],    # Map 'question' key to 'input' (or 'query' depending on the evaluator's prompt)\n",
        "    }\n",
        "\n",
        "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\" : eval_llm},prepare_data=prep_data)\n",
        "\n",
        "labeled_helpfulness_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_criteria\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"helpfulness\": (\n",
        "                \"Is this submission helpful to the user,\"\n",
        "                \" taking into account the correct reference answer?\"\n",
        "            )\n",
        "        },\n",
        "        \"llm\" : eval_llm\n",
        "    },\n",
        "    prepare_data=prep_data\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiJL9ap7h7z4"
      },
      "outputs": [],
      "source": [
        "evaluate(\n",
        "    base_rag_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator\n",
        "\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"Base_Embedding_Model\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3J2gZych-xu"
      },
      "outputs": [],
      "source": [
        "evaluate(\n",
        "    finetune_rag_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"Finetune_Embedding_Model\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxj-t2KlKxsT"
      },
      "source": [
        "<span style=\"color:green\">**Langsmith Evaluation Results: Much better!**\n",
        "\n",
        "<span style=\"color:green\"> Bingo! Langsmith evaluation shows that our fine tuned model got better scores. I tried three runs and the results were the same in each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3upjDhlKxsT"
      },
      "source": [
        "![image](ls_eval.png)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}